{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework11_selfsupervised2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!if [ -d dul_2021 ]; then rm -Rf dul_2021; fi\n",
        "!git clone https://github.com/GrigoryBartosh/dul_2021\n",
        "!pip install ./dul_2021"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dRz5SIFKfZD",
        "outputId": "cc52351b-70bd-41e9-f4c3-6b55aa0e5efd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dul_2021'...\n",
            "remote: Enumerating objects: 361, done.\u001b[K\n",
            "remote: Counting objects: 100% (198/198), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 361 (delta 112), reused 91 (delta 65), pack-reused 163\u001b[K\n",
            "Receiving objects: 100% (361/361), 55.77 MiB | 18.99 MiB/s, done.\n",
            "Resolving deltas: 100% (169/169), done.\n",
            "Processing ./dul_2021\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Building wheels for collected packages: dul-2021\n",
            "  Building wheel for dul-2021 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dul-2021: filename=dul_2021-0.1.0-py3-none-any.whl size=26856 sha256=54b579c92752f5b2453b76c5d1829c3efec02c5a47e429a5718e780e5d4b3408\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p7raf333/wheels/55/59/29/0fb1c635652157734f4d741f32fc11979149684e83e919de06\n",
            "Successfully built dul-2021\n",
            "Installing collected packages: dul-2021\n",
            "Successfully installed dul-2021-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dul_2021.utils.hw11_utils import *"
      ],
      "metadata": {
        "id": "KjY-iIy5MSZb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1. BYOL\n",
        "\n",
        "Here we will implement [BYOL](https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "* You can combine view, representation, and projection into one network. You can use same architechure as in practice. \n",
        "\n",
        "* Use BatchNorm\n",
        "\n",
        "* As predictor use few linear layers\n",
        "\n",
        "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "* Ï„ = 0.99 (target update coefficient)\n",
        "* lr = 1e-4\n",
        "* num_epochs = 20\n",
        "* latent dim = 128\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "t7J5FgOHW6Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "ygRQxlah7nDO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, out_dim=128, hid_dim_full=128):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "        self.conv2 = nn.Conv2d(16, 16, 3, padding=1, stride=2)\n",
        "        self.conv3 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "        self.conv4 = nn.Conv2d(32, 32, 3, padding=1, stride=2)\n",
        "        self.conv5 = nn.Conv2d(32, 32, 1)\n",
        "        self.conv6 = nn.Conv2d(32, 4, 1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "        self.bn5 = nn.BatchNorm2d(32)\n",
        "        self.bn6 = nn.BatchNorm2d(4)\n",
        "\n",
        "        self.conv_to_fc = 7 * 7 * 4\n",
        "        self.fc1 = nn.Linear(self.conv_to_fc, hid_dim_full)\n",
        "        self.fc2 = nn.Linear(hid_dim_full, int(hid_dim_full // 2))\n",
        "\n",
        "        self.features = nn.Linear(int(hid_dim_full // 2), out_dim)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "    \n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.bn2(self.conv2(x)))\n",
        "        x = F.relu(self.bn3(self.conv3(x)))\n",
        "        x = F.relu(self.bn4(self.conv4(x)))\n",
        "        x = F.relu(self.bn5(self.conv5(x)))\n",
        "        x = F.relu(self.bn6(self.conv6(x)))\n",
        "\n",
        "        x = x.view(-1, self.conv_to_fc)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        features = self.features(x)\n",
        "\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "3B1F1aoNBAKI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, hd=256, latent_size=128, tau=0.99):\n",
        "        super().__init__()\n",
        "        self.teacher = Net()\n",
        "        self.student = copy.deepcopy(self.teacher)\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(latent_size, hd), nn.BatchNorm1d(hd), nn.ReLU(),\n",
        "            nn.Linear(hd, hd), nn.BatchNorm1d(hd), nn.ReLU(),\n",
        "            nn.Linear(hd, latent_size)\n",
        "        )\n",
        "        self.tau = tau\n",
        "\n",
        "        for param in self.teacher.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    def update(self):\n",
        "        for param_t, param_s in zip(self.teacher.parameters(), self.student.parameters()):\n",
        "            param_t.data = self.tau * param_t.data + (1 - self.tau) * param_s.data\n",
        "\n",
        "    def similarity(self, x, y):\n",
        "        q_z_t = self.predictor(self.teacher(x))\n",
        "        with torch.no_grad():\n",
        "            z_s = self.student(y).detach()\n",
        "\n",
        "        cosine = torch.sum(q_z_t * z_s, dim=1) / (torch.norm(q_z_t, dim=1) * torch.norm(z_s, dim=1))\n",
        "        out = 2 - 2 * cosine\n",
        "        return out.mean()\n",
        "\n",
        "    def loss(self, x, y):\n",
        "        return self.similarity(x, y) + self.similarity(y, x)\n",
        "\n",
        "    def representations(self, x):\n",
        "        x = x.to(device)\n",
        "        self.student.eval()\n",
        "        with torch.no_grad():\n",
        "            out = self.student(x).detach()\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "-hd2AnEhvRvd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "\n",
        "def train(model, train_data, epochs=10, lr=1e-3, bs=128):\n",
        "    opt = Adam(model.parameters(), lr=lr)\n",
        "    dataloader = DataLoader(train_data, batch_size=bs, shuffle=True)\n",
        "    losses = []\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for x, y in iter(dataloader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            loss = model.loss(x, y)\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            model.update()\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    return np.array(losses)"
      ],
      "metadata": {
        "id": "-Ld-bHkHgEjq"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import ToPILImage\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomResizedCrop(size=28),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.dataset[idx][0].add(1).sub(2)\n",
        "        x1 = self.transforms(x)\n",
        "        x2 = self.transforms(x)\n",
        "        return x1, x2"
      ],
      "metadata": {
        "id": "tu0AlzR7HXLw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q1(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 28, 28) torchvision dataset of MNIST images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\"\n",
        "    dataset = CustomDataset(train_data)\n",
        "    model = Model().to(device)\n",
        "\n",
        "    losses = train(model, dataset, epochs=10)\n",
        "\n",
        "    return losses, model.representations"
      ],
      "metadata": {
        "id": "Og9Fv7sV6nrO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
        "q1_results(q1, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "3Ib0ne2mX80s",
        "outputId": "bc6c4126-48fe-4d30-a5c2-bab6ec880eae"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [14:19<00:00, 85.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean classification accuracy=0.5443\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hyjVgSxqX6qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2. Barlow Twins\n",
        "\n",
        "Here we will implement [barlow twins](https://arxiv.org/abs/2103.03230).\n",
        "\n",
        "* You can use same architechure as in practice. \n",
        "\n",
        "* Dataset comes untransformed, so you need to apply transformations during training by yourself. Use same augmentations as in SimCLR\n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "* Î» = 0.01 \n",
        "* lr = 5e-4\n",
        "* num_epochs = 20\n",
        "* latent dim = 128\n",
        "\n",
        "\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "hlbaIthyMGab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def q2(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "pd6RrZfP75HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change flag to False if you want only to test your losses w/o accuracy (it takes around 4-5 minutes)\n",
        "q2_results(q2, True)"
      ],
      "metadata": {
        "id": "EA1Z_s1a8_sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus. SwAV\n",
        "\n",
        "Here we will implement [SwAV](https://arxiv.org/abs/2006.09882v5)\n",
        "\n",
        "**You will provide the following deliverables**\n",
        "\n",
        "\n",
        "1. Over the course of training, record loss ber batch.\n",
        "2. A function that encodes a batch of images with your trained model. The function recieves a batch torch tensors on cpu and should return transformed 2d tensor (batch size x laten dim). It will be used to test representation on classification task."
      ],
      "metadata": {
        "id": "D8UN9nr9aYGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def b(train_data):\n",
        "    \"\"\"\n",
        "    train_data: An (n_train, 1, 32, 32) torchvision dataset of CIFAR10 images with values from -1 to 1\n",
        "\n",
        "    Returns\n",
        "    - a (# of training iterations, ) numpy array  losses on each iteration\n",
        "    - a function that transforms batch of images into their latent representation\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "1Yin_8Ebaa8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q2_results(b, True)"
      ],
      "metadata": {
        "id": "S0jgIgLN8tzd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}